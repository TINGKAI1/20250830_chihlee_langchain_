{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f25724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ AI å›æ‡‰ï¼š\n",
      "{'model': 'gemma3:1b', 'created_at': '2025-09-13T07:59:16.8007572Z', 'response': '{\\n\"ç°¡æ½”è§£é‡‹\": [\\n\"Pythonçš„å‡½å¼å°±åƒæ˜¯ç¨‹å¼ç¢¼çš„å°ä»»å‹™ï¼Œå®ƒå€‘æœƒåšä¸€äº›ç‰¹å®šçš„äº‹æƒ…ã€‚\",\\n\"å®ƒå€‘åƒæ˜¯æŒ‡ä»¤ï¼Œå¯ä»¥è¢«ç¨‹å¼ç¢¼åŸ·è¡Œã€‚\",\\n\"å‡½å¼å¯ä»¥é‡è¤‡åŸ·è¡Œï¼Œå°±åƒæ˜¯ç¨‹å¼ç¢¼ä¸­çš„è¿´åœˆï¼Œè®“ç¨‹å¼ç¢¼å¯ä»¥è‡ªå‹•åŸ·è¡Œã€‚\",\\n\"å®ƒå€‘æœ‰åå­—ï¼Œæ–¹ä¾¿æˆ‘å€‘ç†è§£å’Œç®¡ç†ç¨‹å¼ç¢¼ã€‚\",\\n\"å‡½å¼å¯ä»¥æ¥å—è¼¸å…¥ï¼ˆåƒæ•¸ï¼‰ï¼Œä¸¦ç”¢ç”Ÿè¼¸å‡ºï¼ˆè¿”å›å€¼ï¼‰ã€‚\"\\n]\\n}', 'done': True, 'done_reason': 'stop', 'context': [105, 2364, 107, 239230, 237105, 74624, 48280, 185411, 26549, 237026, 32651, 236918, 238780, 237522, 237536, 106, 107, 105, 4368, 107, 236782, 107, 236775, 239309, 241910, 185411, 1083, 870, 107, 236775, 32651, 236918, 238780, 237522, 55326, 237026, 47972, 240897, 33292, 152807, 236900, 162576, 238003, 237893, 18303, 172612, 25606, 69079, 107, 236775, 162576, 95714, 81582, 236900, 5157, 237759, 47972, 240897, 78204, 69079, 107, 236775, 238780, 237522, 5157, 174322, 78204, 236900, 55326, 237026, 47972, 240897, 12870, 244125, 239866, 236900, 239301, 47972, 240897, 5157, 39972, 78204, 69079, 107, 236775, 162576, 237078, 46075, 236900, 44128, 13125, 29666, 237206, 12071, 47972, 240897, 69079, 107, 236775, 238780, 237522, 5157, 38313, 62588, 237221, 209458, 19612, 238953, 125716, 185992, 237221, 195780, 21719, 236775, 107, 236842, 107, 236783], 'total_duration': 2366580900, 'load_duration': 1333364100, 'prompt_eval_count': 21, 'prompt_eval_duration': 180228900, 'eval_count': 100, 'eval_duration': 724693300}\n",
      "{\n",
      "\"ç°¡æ½”è§£é‡‹\": [\n",
      "\"Pythonçš„å‡½å¼å°±åƒæ˜¯ç¨‹å¼ç¢¼çš„å°ä»»å‹™ï¼Œå®ƒå€‘æœƒåšä¸€äº›ç‰¹å®šçš„äº‹æƒ…ã€‚\",\n",
      "\"å®ƒå€‘åƒæ˜¯æŒ‡ä»¤ï¼Œå¯ä»¥è¢«ç¨‹å¼ç¢¼åŸ·è¡Œã€‚\",\n",
      "\"å‡½å¼å¯ä»¥é‡è¤‡åŸ·è¡Œï¼Œå°±åƒæ˜¯ç¨‹å¼ç¢¼ä¸­çš„è¿´åœˆï¼Œè®“ç¨‹å¼ç¢¼å¯ä»¥è‡ªå‹•åŸ·è¡Œã€‚\",\n",
      "\"å®ƒå€‘æœ‰åå­—ï¼Œæ–¹ä¾¿æˆ‘å€‘ç†è§£å’Œç®¡ç†ç¨‹å¼ç¢¼ã€‚\",\n",
      "\"å‡½å¼å¯ä»¥æ¥å—è¼¸å…¥ï¼ˆåƒæ•¸ï¼‰ï¼Œä¸¦ç”¢ç”Ÿè¼¸å‡ºï¼ˆè¿”å›å€¼ï¼‰ã€‚\"\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #åƒè€ƒèªªæ˜1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"ğŸ’¬ AI å›æ‡‰ï¼š\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "#ç¯„ä¾‹è¼¸å…¥\n",
    "chat_with_ollama(\"è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17606cb3",
   "metadata": {},
   "source": [
    "æ¨¡å‹ç”Ÿæˆæ–‡å­—æ™‚çš„å¸¸è¦‹è¶…åƒæ•¸æœ‰ä¸‰å€‹ï¼štemperatureã€top_pã€top_kï¼Œå®ƒå€‘ä¸»è¦ç”¨ä¾†æ§åˆ¶ç”Ÿæˆæ–‡æœ¬çš„å¤šæ¨£æ€§èˆ‡éš¨æ©Ÿæ€§ã€‚\n",
    "\n",
    "### temperature (æº«åº¦)\n",
    "- ç”¨ä¾†èª¿æ•´æ¨¡å‹è©æ¦‚ç‡åˆ†å¸ƒçš„ã€Œå¹³æ»‘åº¦ã€ã€‚\n",
    "- æº«åº¦è¶Šä½ï¼ˆæ¥è¿‘0ï¼‰ï¼Œæ¨¡å‹æ›´å‚¾å‘é¸æ“‡æ©Ÿç‡æœ€é«˜çš„è©ï¼Œç”¢å‡ºè¼ƒä¿å®ˆã€ç©©å®šã€é‡è¤‡æ€§é«˜çš„æ–‡å­—ã€‚\n",
    "- æº«åº¦è¶Šé«˜ï¼ˆé€šå¸¸>1ï¼‰ï¼Œæ¨¡å‹æ›´å®¹æ˜“é¸æ“‡ä½æ©Ÿç‡è©ï¼Œç”¢å‡ºæ›´æœ‰å‰µæ„å’Œå¤šæ¨£æ€§çš„æ–‡å­—ï¼Œä½†å“è³ªå¯èƒ½ä¸ç©©å®šã€‚\n",
    "- ç¯„ä¾‹ï¼štemperature=0.1æ™‚å¸¸ç”¢ç”Ÿç›¸ä¼¼å¥å­ï¼Œtemperature=1.0å‰‡å¤šæ¨£æ€§å¤§å¹…æå‡ã€‚\n",
    "\n",
    "### top_k\n",
    "- æ¨¡å‹åªå¾å‰kå€‹æœ€é«˜æ©Ÿç‡çš„è©ä¸­é¸æ“‡ä¸‹ä¸€å€‹è©ã€‚\n",
    "- kå€¼è¶Šå°ï¼Œç¯„åœè¶Šçª„ï¼Œç”Ÿæˆå…§å®¹è¶Šé›†ä¸­å’Œå¯é æ¸¬ã€‚\n",
    "- kå€¼è¶Šå¤§ï¼Œæ¨¡å‹èƒ½è€ƒæ…®æ›´å¤šè©ï¼Œå¢åŠ å¤šæ¨£æ€§ï¼Œä½†å¯èƒ½å°è‡´ç”¢å‡ºè¼ƒä¸é€£è²«æˆ–å¥‡æ€ªçš„è©ã€‚\n",
    "- ç¯„ä¾‹ï¼štop_k=5æ™‚åªè€ƒæ…®5å€‹è©ï¼Œtop_k=50å‰‡è€ƒæ…®æ›´å¤šè©ã€‚\n",
    "\n",
    "### top_p (åˆç¨±Nucleus sampling)\n",
    "- ä¸æ˜¯é™åˆ¶è©æ•¸ï¼Œè€Œæ˜¯é¸æ“‡ç´¯ç©æ©Ÿç‡ç¸½å’Œé”pçš„è©é›†åˆã€‚\n",
    "- pè¶Šå°ï¼Œå€™é¸è©é›†åˆè¶Šå°ï¼ˆæ›´é›†ä¸­ï¼‰ï¼Œpè¶Šå¤§å‰‡åŒ…å«æ›´å¤šè©ã€‚\n",
    "- åœ¨top_pä¸‹ï¼Œå€™é¸è©æ•¸ç›®æœƒæ ¹æ“šæ©Ÿç‡åˆ†ä½ˆå‹•æ…‹èª¿æ•´ï¼Œè¼ƒéˆæ´»ã€‚\n",
    "- ç¯„ä¾‹ï¼štop_p=0.9è¡¨ç¤ºå¾æ©Ÿç‡ç´¯ç©é”90%çš„è©ä¸­é¸æ“‡ã€‚\n",
    "\n",
    "é€™ä¸‰å€‹è¶…åƒæ•¸é€šå¸¸é…åˆä½¿ç”¨ï¼Œå¯ä»¥é€éèª¿æ•´ä¾†æ§åˆ¶ç”Ÿæˆå…§å®¹çš„é¢¨æ ¼å’Œå“è³ªã€‚ä¾‹å¦‚ï¼š\n",
    "\n",
    "```python\n",
    "temperature = 0.7\n",
    "top_k = 40\n",
    "top_p = 0.9\n",
    "```\n",
    "\n",
    "é€™æ˜¯ä¸€çµ„å¸¸è¦‹çš„åƒæ•¸é…ç½®ï¼Œä½¿æ–‡æœ¬æ—¢ä¸éæ–¼æ­»æ¿ï¼Œä¹Ÿä¸æœƒå¤ªéæ•£äº‚ï¼Œæœ‰é©åº¦çš„å‰µé€ æ€§å’Œé€£è²«æ€§[7][4][10].\n",
    "\n",
    "ä¾†æº\n",
    "[1] æ·ºè«‡LLM å¤§å‹èªè¨€æ¨¡å‹çš„Temperatureã€Top-P å’ŒTop-K åƒæ•¸åˆ†äº« https://blog.miniasp.com/post/2024/05/21/LLM-Temperature-Top-P-Nucleus-Sampling-Top-K\n",
    "[2] å¤§æ¨¡å‹åŠ è½½çš„å‚æ•°ä»‹ç»åŠæ¨èè¡¨ï¼Œtemperatureã€top_kã€top_p https://blog.csdn.net/a1920993165/article/details/134691021\n",
    "[3] NLP / LLMsä¸­çš„Temperature æ˜¯ä»€ä¹ˆ? åŸåˆ› - CSDNåšå®¢ https://blog.csdn.net/deephub/article/details/129682591\n",
    "[4] å¤§æ¨¡å‹ç”Ÿæˆç­–ç•¥å‚æ•°è¯¦è§£ï¼šTop-Kã€Top-P å’ŒTemperature åŸåˆ› https://blog.csdn.net/qq_35971258/article/details/143753893\n",
    "[5] LLM è¶…åƒæ•¸è¨­å®š https://learnprompting.org/zh-tw/docs/intermediate/configuration_hyperparameters\n",
    "[6] LLMæ¢ç´¢ï¼šGPTç±»æ¨¡å‹çš„å‡ ä¸ªå¸¸ç”¨å‚æ•° Top-k, Top-p, Temperature https://www.cnblogs.com/deali/p/llm-2.html\n",
    "[7] AIå¤§è¯­è¨€æ¨¡å‹çš„æ¸©åº¦ã€top_kç­‰è¶…å‚æ•°æ€ä¹ˆç†è§£ - CSDNåšå®¢ https://blog.csdn.net/weixin_41736460/article/details/139558975\n",
    "[8] å¤§æ¨¡å‹åŸºç¡€æ¦‚å¿µä¹‹Top-kã€Top-p ç­‰å‚æ•° - tinywell http://tinywell.com/2024/05/15/llm-params/\n",
    "[9] åº¶æ°‘èªè¨€èªªOpenAIè£¡é¢Temperature è·ŸTop_p åƒæ•¸( ... https://vocus.cc/article/665ee3e9fd89780001ad34ed\n",
    "[10] LLM ä¸­çš„æº«åº¦ã€Top Pã€Top K æ˜¯ä»€éº¼ï¼Ÿï¼ˆå¾æ¦‚å¿µåˆ°ä»£ç¢¼ï¼‰ https://www.toolify.ai/tw/ai-news-tw/llm-%E4%B8%AD%E7%9A%84%E6%BA%AB%E5%BA%A6top-ptop-k-%E6%98%AF%E4%BB%80%E9%BA%BC%E5%BE%9E%E6%A6%82%E5%BF%B5%E5%88%B0%E4%BB%A3%E7%A2%BC-968155\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30395f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¬ AI å›æ‡‰ï¼š\n",
      "{'error': \"model 'gpt-oss:20b' not found\"}\n",
      "âŒ æ²’æœ‰æ‰¾åˆ°å›æ‡‰å…§å®¹\n",
      "å¯ç”¨çš„æ¬„ä½: ['error']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gpt-oss:20b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        # ç§»é™¤ format: \"json\" åƒæ•¸ï¼Œè®“æ¨¡å‹æ­£å¸¸å›æ‡‰\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"ğŸ’¬ AI å›æ‡‰ï¼š\")\n",
    "    print(result)\n",
    "    \n",
    "    if \"response\" in result:\n",
    "        print(f\"\\nğŸ“ å›ç­”å…§å®¹ï¼š\\n{result['response']}\")\n",
    "    else:\n",
    "        print(\"âŒ æ²’æœ‰æ‰¾åˆ°å›æ‡‰å…§å®¹\")\n",
    "        print(\"å¯ç”¨çš„æ¬„ä½:\", list(result.keys()))\n",
    "\n",
    "#ç¯„ä¾‹è¼¸å…¥\n",
    "chat_with_ollama(\"è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93ad8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤– æ¸¬è©¦æ¨¡å‹: gemma3:1b\n",
      "==================================================\n",
      "âœ… gemma3:1b å›æ‡‰æ­£å¸¸\n",
      "ğŸ“ å›ç­”: å¥½çš„ï¼Œæˆ‘ä¾†ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ Python çš„å‡½å¼ï¼š\n",
      "\n",
      "**æƒ³åƒä¸€ä¸‹ï¼ŒPython çš„å‡½å¼å°±åƒä¸€å€‹å·¥å…·ç®±ï¼Œè£¡é¢æœ‰å„ç¨®å„æ¨£çš„å·¥å…·ï¼Œå¯ä»¥åšä¸åŒçš„äº‹æƒ…ã€‚**\n",
      "\n",
      "* **å‡½å¼æ˜¯ç¨‹å¼ç¢¼çš„â€œå°ä»»å‹™â€ã€‚** ç¨‹å¼ç¢¼çš„ç›®æ¨™æ˜¯è®“é›»è…¦åŸ·è¡Œç‰¹å®šçš„å‹•ä½œï¼Œä¾‹å¦‚è¨ˆç®—ã€é¡¯ç¤ºè¨Šæ¯ã€è®€å–è³‡æ–™ç­‰ç­‰ã€‚\n",
      "* **å‡½å¼æ˜¯é‡è¤‡åŸ·è¡Œçš„æ“ä½œã€‚** å°±åƒä¸€å€‹ç¨‹å¼ç¢¼çš„â€œæŒ‡ä»¤â€ï¼Œå¯ä»¥å¤šæ¬¡åœ°åŸ·è¡Œã€‚\n",
      "* **å‡½å¼æœ‰ä¸åŒçš„â€œåŠŸèƒ½â€ï¼Œå®ƒå€‘å¯ä»¥ï¼š**\n",
      "    *...\n",
      "\n",
      "ğŸ¤– æ¸¬è©¦æ¨¡å‹: gpt-oss:20b\n",
      "==================================================\n",
      "âŒ gpt-oss:20b å›æ‡‰ç•°å¸¸\n",
      "ğŸ” å®Œæ•´å›æ‡‰: {'error': \"model 'gpt-oss:20b' not found\"}\n"
     ]
    }
   ],
   "source": [
    "# æ¯”è¼ƒå…©å€‹æ¨¡å‹çš„å›æ‡‰\n",
    "import requests\n",
    "\n",
    "def test_model(model_name, prompt):\n",
    "    print(f\"\\nğŸ¤– æ¸¬è©¦æ¨¡å‹: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        result = response.json()\n",
    "        \n",
    "        if \"response\" in result and result[\"response\"].strip():\n",
    "            print(f\"âœ… {model_name} å›æ‡‰æ­£å¸¸\")\n",
    "            print(f\"ğŸ“ å›ç­”: {result['response'][:200]}...\")\n",
    "        else:\n",
    "            print(f\"âŒ {model_name} å›æ‡‰ç•°å¸¸\")\n",
    "            print(f\"ğŸ” å®Œæ•´å›æ‡‰: {result}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model_name} ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "\n",
    "# æ¸¬è©¦å…©å€‹æ¨¡å‹\n",
    "prompt = \"è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ\"\n",
    "test_model(\"gemma3:1b\", prompt)\n",
    "test_model(\"gpt-oss:20b\", prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
