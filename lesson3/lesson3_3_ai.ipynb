{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f25724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 AI 回應：\n",
      "{'model': 'gemma3:1b', 'created_at': '2025-09-13T07:59:16.8007572Z', 'response': '{\\n\"簡潔解釋\": [\\n\"Python的函式就像是程式碼的小任務，它們會做一些特定的事情。\",\\n\"它們像是指令，可以被程式碼執行。\",\\n\"函式可以重複執行，就像是程式碼中的迴圈，讓程式碼可以自動執行。\",\\n\"它們有名字，方便我們理解和管理程式碼。\",\\n\"函式可以接受輸入（參數），並產生輸出（返回值）。\"\\n]\\n}', 'done': True, 'done_reason': 'stop', 'context': [105, 2364, 107, 239230, 237105, 74624, 48280, 185411, 26549, 237026, 32651, 236918, 238780, 237522, 237536, 106, 107, 105, 4368, 107, 236782, 107, 236775, 239309, 241910, 185411, 1083, 870, 107, 236775, 32651, 236918, 238780, 237522, 55326, 237026, 47972, 240897, 33292, 152807, 236900, 162576, 238003, 237893, 18303, 172612, 25606, 69079, 107, 236775, 162576, 95714, 81582, 236900, 5157, 237759, 47972, 240897, 78204, 69079, 107, 236775, 238780, 237522, 5157, 174322, 78204, 236900, 55326, 237026, 47972, 240897, 12870, 244125, 239866, 236900, 239301, 47972, 240897, 5157, 39972, 78204, 69079, 107, 236775, 162576, 237078, 46075, 236900, 44128, 13125, 29666, 237206, 12071, 47972, 240897, 69079, 107, 236775, 238780, 237522, 5157, 38313, 62588, 237221, 209458, 19612, 238953, 125716, 185992, 237221, 195780, 21719, 236775, 107, 236842, 107, 236783], 'total_duration': 2366580900, 'load_duration': 1333364100, 'prompt_eval_count': 21, 'prompt_eval_duration': 180228900, 'eval_count': 100, 'eval_duration': 724693300}\n",
      "{\n",
      "\"簡潔解釋\": [\n",
      "\"Python的函式就像是程式碼的小任務，它們會做一些特定的事情。\",\n",
      "\"它們像是指令，可以被程式碼執行。\",\n",
      "\"函式可以重複執行，就像是程式碼中的迴圈，讓程式碼可以自動執行。\",\n",
      "\"它們有名字，方便我們理解和管理程式碼。\",\n",
      "\"函式可以接受輸入（參數），並產生輸出（返回值）。\"\n",
      "]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gemma3:1b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": { #參考說明1\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        },\n",
    "        \"max_tokens\": 100,\n",
    "        \"format\": \"json\",\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"💬 AI 回應：\")\n",
    "    # Print the whole result for debugging\n",
    "    print(result)\n",
    "    # Try to print the 'response' key if it exists, otherwise print possible keys\n",
    "    if \"response\" in result:\n",
    "        print(result[\"response\"])\n",
    "    elif \"message\" in result:\n",
    "        print(result[\"message\"])\n",
    "    elif \"content\" in result:\n",
    "        print(result[\"content\"])\n",
    "    else:\n",
    "        print(\"No expected key found in response. Available keys:\", result.keys())\n",
    "\n",
    "#範例輸入\n",
    "chat_with_ollama(\"請用簡單的方式解釋什麼是Python的函式？\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17606cb3",
   "metadata": {},
   "source": [
    "模型生成文字時的常見超參數有三個：temperature、top_p、top_k，它們主要用來控制生成文本的多樣性與隨機性。\n",
    "\n",
    "### temperature (溫度)\n",
    "- 用來調整模型詞概率分布的「平滑度」。\n",
    "- 溫度越低（接近0），模型更傾向選擇機率最高的詞，產出較保守、穩定、重複性高的文字。\n",
    "- 溫度越高（通常>1），模型更容易選擇低機率詞，產出更有創意和多樣性的文字，但品質可能不穩定。\n",
    "- 範例：temperature=0.1時常產生相似句子，temperature=1.0則多樣性大幅提升。\n",
    "\n",
    "### top_k\n",
    "- 模型只從前k個最高機率的詞中選擇下一個詞。\n",
    "- k值越小，範圍越窄，生成內容越集中和可預測。\n",
    "- k值越大，模型能考慮更多詞，增加多樣性，但可能導致產出較不連貫或奇怪的詞。\n",
    "- 範例：top_k=5時只考慮5個詞，top_k=50則考慮更多詞。\n",
    "\n",
    "### top_p (又稱Nucleus sampling)\n",
    "- 不是限制詞數，而是選擇累積機率總和達p的詞集合。\n",
    "- p越小，候選詞集合越小（更集中），p越大則包含更多詞。\n",
    "- 在top_p下，候選詞數目會根據機率分佈動態調整，較靈活。\n",
    "- 範例：top_p=0.9表示從機率累積達90%的詞中選擇。\n",
    "\n",
    "這三個超參數通常配合使用，可以透過調整來控制生成內容的風格和品質。例如：\n",
    "\n",
    "```python\n",
    "temperature = 0.7\n",
    "top_k = 40\n",
    "top_p = 0.9\n",
    "```\n",
    "\n",
    "這是一組常見的參數配置，使文本既不過於死板，也不會太過散亂，有適度的創造性和連貫性[7][4][10].\n",
    "\n",
    "來源\n",
    "[1] 淺談LLM 大型語言模型的Temperature、Top-P 和Top-K 參數分享 https://blog.miniasp.com/post/2024/05/21/LLM-Temperature-Top-P-Nucleus-Sampling-Top-K\n",
    "[2] 大模型加载的参数介绍及推荐表，temperature、top_k、top_p https://blog.csdn.net/a1920993165/article/details/134691021\n",
    "[3] NLP / LLMs中的Temperature 是什么? 原创 - CSDN博客 https://blog.csdn.net/deephub/article/details/129682591\n",
    "[4] 大模型生成策略参数详解：Top-K、Top-P 和Temperature 原创 https://blog.csdn.net/qq_35971258/article/details/143753893\n",
    "[5] LLM 超參數設定 https://learnprompting.org/zh-tw/docs/intermediate/configuration_hyperparameters\n",
    "[6] LLM探索：GPT类模型的几个常用参数 Top-k, Top-p, Temperature https://www.cnblogs.com/deali/p/llm-2.html\n",
    "[7] AI大语言模型的温度、top_k等超参数怎么理解 - CSDN博客 https://blog.csdn.net/weixin_41736460/article/details/139558975\n",
    "[8] 大模型基础概念之Top-k、Top-p 等参数 - tinywell http://tinywell.com/2024/05/15/llm-params/\n",
    "[9] 庶民語言說OpenAI裡面Temperature 跟Top_p 參數( ... https://vocus.cc/article/665ee3e9fd89780001ad34ed\n",
    "[10] LLM 中的溫度、Top P、Top K 是什麼？（從概念到代碼） https://www.toolify.ai/tw/ai-news-tw/llm-%E4%B8%AD%E7%9A%84%E6%BA%AB%E5%BA%A6top-ptop-k-%E6%98%AF%E4%BB%80%E9%BA%BC%E5%BE%9E%E6%A6%82%E5%BF%B5%E5%88%B0%E4%BB%A3%E7%A2%BC-968155\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30395f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💬 AI 回應：\n",
      "{'error': \"model 'gpt-oss:20b' not found\"}\n",
      "❌ 沒有找到回應內容\n",
      "可用的欄位: ['error']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def chat_with_ollama(prompt: str):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"gpt-oss:20b\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        # 移除 format: \"json\" 參數，讓模型正常回應\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, json=payload)\n",
    "    result = response.json()\n",
    "    print(\"💬 AI 回應：\")\n",
    "    print(result)\n",
    "    \n",
    "    if \"response\" in result:\n",
    "        print(f\"\\n📝 回答內容：\\n{result['response']}\")\n",
    "    else:\n",
    "        print(\"❌ 沒有找到回應內容\")\n",
    "        print(\"可用的欄位:\", list(result.keys()))\n",
    "\n",
    "#範例輸入\n",
    "chat_with_ollama(\"請用簡單的方式解釋什麼是Python的函式？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93ad8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 測試模型: gemma3:1b\n",
      "==================================================\n",
      "✅ gemma3:1b 回應正常\n",
      "📝 回答: 好的，我來用簡單的方式解釋 Python 的函式：\n",
      "\n",
      "**想像一下，Python 的函式就像一個工具箱，裡面有各種各樣的工具，可以做不同的事情。**\n",
      "\n",
      "* **函式是程式碼的“小任務”。** 程式碼的目標是讓電腦執行特定的動作，例如計算、顯示訊息、讀取資料等等。\n",
      "* **函式是重複執行的操作。** 就像一個程式碼的“指令”，可以多次地執行。\n",
      "* **函式有不同的“功能”，它們可以：**\n",
      "    *...\n",
      "\n",
      "🤖 測試模型: gpt-oss:20b\n",
      "==================================================\n",
      "❌ gpt-oss:20b 回應異常\n",
      "🔍 完整回應: {'error': \"model 'gpt-oss:20b' not found\"}\n"
     ]
    }
   ],
   "source": [
    "# 比較兩個模型的回應\n",
    "import requests\n",
    "\n",
    "def test_model(model_name, prompt):\n",
    "    print(f\"\\n🤖 測試模型: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"temperature\": 0.7,\n",
    "            \"top_p\": 0.9,\n",
    "            \"top_k\": 50,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        result = response.json()\n",
    "        \n",
    "        if \"response\" in result and result[\"response\"].strip():\n",
    "            print(f\"✅ {model_name} 回應正常\")\n",
    "            print(f\"📝 回答: {result['response'][:200]}...\")\n",
    "        else:\n",
    "            print(f\"❌ {model_name} 回應異常\")\n",
    "            print(f\"🔍 完整回應: {result}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ {model_name} 發生錯誤: {e}\")\n",
    "\n",
    "# 測試兩個模型\n",
    "prompt = \"請用簡單的方式解釋什麼是Python的函式？\"\n",
    "test_model(\"gemma3:1b\", prompt)\n",
    "test_model(\"gpt-oss:20b\", prompt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
